{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Objective\n",
    "\n",
    "To have a machine-readable set of data on the State Records Office's collection of scanned maps and survey plans, with\n",
    "each object having a full set of metadata available, including a rough spatial location to enable end-users to\n",
    "choose objects of local interest to georectify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For want of an API, let's scrape the digital objects collection from SRO.\n",
    "\n",
    "https://github.com/geogeeks-au/maps-for-lost-towns/issues/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import os.path\n",
    "from pprintpp import pprint as pp\n",
    "\n",
    "SRO_CATALOGUE_URL = \"https://archive.sro.wa.gov.au\"\n",
    "page = 225\n",
    "scraping = True\n",
    "json_file = \"sro_digital_objects_collection.json\"\n",
    "digital_objects = []\n",
    "\n",
    "while scraping is True:\n",
    "    # @TODO Determine what a nice wait period is to stop SRO's server from falling over\n",
    "    time.sleep(1)\n",
    "    \n",
    "    r = requests.get(\n",
    "        SRO_CATALOGUE_URL + \"/index.php/digitalobject/browse?page=%s&limit=30&sort=identifier\" % (page)\n",
    "    )\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    anchors = soup.select(\"div.preview > a\")\n",
    "        \n",
    "    if len(anchors) == 0:\n",
    "        scraping = False\n",
    "    else:\n",
    "        print \"# Page #%s, #%s Items\" % (page, len(anchors))\n",
    "#         print\n",
    "        \n",
    "        digital_objects = []\n",
    "        \n",
    "        for anchor in anchors:\n",
    "#             print \"Thumbnail\"\n",
    "#             print SRO_CATALOGUE_URL + anchor.contents[0][\"src\"]\n",
    "#             print\n",
    "\n",
    "            print SRO_CATALOGUE_URL + anchor[\"href\"]\n",
    "            r_item = requests.get(\"https://archive.sro.wa.gov.au%s\" % (anchor[\"href\"]))\n",
    "            item_soup = BeautifulSoup(r_item.text, \"lxml\")\n",
    "\n",
    "            breadcrumbs = item_soup.select(\"section.breadcrumb > ul > li > a\")\n",
    "#             print breadcrumbs[0].text\n",
    "#             print\n",
    "\n",
    "            fields = item_soup.select(\"div#content > section > div.field\")\n",
    "#             print \"Num. Fields: %s\" % (len(fields))\n",
    "#             print\n",
    "\n",
    "            fields_santised = {\"name_of_creator\": []}\n",
    "            for field in fields:\n",
    "                field_name = field.find(\"h3\").text.strip().lower().replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"(\", \"_\").replace(\")\", \"_\")\n",
    "                field_value = field.find(\"div\").text.replace(\"\\n\", \"\").strip()\n",
    "\n",
    "                # Ignore the crap tonne of the empty fields hidden\n",
    "                if field_value != \"\":\n",
    "                    # Some field names reoccur (e.g. Name of creator).\n",
    "                    # Make these an array for happy fun times.\n",
    "#                     if field_name in fields_santised:\n",
    "#                         fields_santised[field_name] = [fields_santised[field_name]]\n",
    "\n",
    "                    if field_name in fields_santised and type(fields_santised[field_name]) is list:\n",
    "                        fields_santised[field_name].append(field_value)\n",
    "                    else:\n",
    "                        fields_santised[field_name] = field_value\n",
    "\n",
    "#                     print \"%s: '%s'\" % (field_name, field_value)\n",
    "        #             print\n",
    "\n",
    "            digital_object = {\n",
    "                \"object_url\": SRO_CATALOGUE_URL + anchor[\"href\"],\n",
    "                \"thumbnail_url\": SRO_CATALOGUE_URL + anchor.contents[0][\"src\"],\n",
    "                \"series\": breadcrumbs[0].text,\n",
    "                \"fields\": fields_santised\n",
    "            }\n",
    "\n",
    "#             print\n",
    "#             pp(digital_object)\n",
    "\n",
    "            digital_objects.append(digital_object)\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        if os.path.isfile(json_file):\n",
    "            with open(json_file) as infile:\n",
    "                digital_objects_json = json.load(infile)\n",
    "    #         pp(digital_objects_json)\n",
    "        else:\n",
    "            digital_objects_json = []\n",
    "        \n",
    "        with open(json_file, \"w\") as outfile:\n",
    "            json.dump(digital_objects_json + digital_objects, outfile)\n",
    "            \n",
    "        page += 1\n",
    "        print\n",
    "    \n",
    "#     scraping = False\n",
    "    \n",
    "    if page > 225:\n",
    "        print \"Fin\"\n",
    "        scraping = False\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load our looted objects into PostgreSQL\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "connection = \"host='pg01.geogeeks.org' dbname='lost_towns' user='keith' password='notmypassword'\"\n",
    "conn = psycopg2.connect(connection)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "with open(json_file) as infile:\n",
    "    digital_objects_json = json.load(infile)\n",
    "\n",
    "for index, dobject in enumerate(digital_objects_json):\n",
    "    print index\n",
    "\n",
    "    query =  (\"INSERT INTO sro_digital_objects_collection (object_url, thumbnail_url, series, fields) \"\n",
    "                  \"VALUES (%s, %s, %s, %s);\")\n",
    "    data = (dobject[\"object_url\"], dobject[\"thumbnail_url\"], dobject[\"series\"], json.dumps(dobject[\"fields\"]))\n",
    "    cursor.execute(query, data)\n",
    "\n",
    "print \"Committing...\"\n",
    "conn.commit()\n",
    "print \"Fin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loot our local cache of images and extract all of the meaningful information we can about them\n",
    "\n",
    "https://github.com/geogeeks-au/maps-for-lost-towns/issues/10\n",
    "    \n",
    "All done! See https://github.com/geogeeks-au/maps-for-lost-towns/blob/master/scrapers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's stitch the two together so we can see if the files we have can be associated with SRO's digital objects collection\n",
    "\n",
    "https://github.com/geogeeks-au/maps-for-lost-towns/issues/11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OK, now finally let's try to assign a spatial location to these\n",
    "\n",
    "https://github.com/geogeeks-au/maps-for-lost-towns/issues/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
